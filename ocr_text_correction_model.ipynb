{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM2snzVDA0YN9zVTAXnqW4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashvoladoddi37/movie-title-ocr-corrector/blob/main/ocr_text_correction_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets pandas scikit-learn tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fp6tt-uGdqe",
        "outputId": "4aee7b06-4fa5-4477-e80c-1ccf381667a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "Successfully installed sympy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA PREPARATION\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "def load_and_prepare_data(test_size=0.2, random_state=42, sample_fraction=0.5):\n",
        "    # Load your dataset\n",
        "    df = pd.read_csv('imdb_title_ocr_variations.csv')\n",
        "\n",
        "    # Reduce dataset size by half\n",
        "    df = df.sample(frac=sample_fraction, random_state=random_state)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Original dataset size: {len(df) / sample_fraction}\")\n",
        "    print(f\"Reduced dataset size: {len(df)}\")\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "    train_df = train_df.rename(columns={'ocr_generated_title': 'incorrect_text'})\n",
        "    val_df = val_df.rename(columns={'ocr_generated_title': 'incorrect_text'})\n",
        "    train_df = train_df.rename(columns={'original_title': 'correct_text'})\n",
        "    val_df = val_df.rename(columns={'original_title': 'correct_text'})\n",
        "\n",
        "    return train_df, val_df"
      ],
      "metadata": {
        "id": "-83tBp4sGUMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE THE TRAINING CLASS\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class OCRTrainer:\n",
        "    def __init__(self, device, use_amp=True, model_name=\"t5-base\"):\n",
        "        self.device = device\n",
        "        self.use_amp = use_amp\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.scaler = GradScaler() if use_amp else None\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, epochs=3, batch_size=32):\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "            for batch in train_pbar:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.use_amp:\n",
        "                    with autocast():\n",
        "                        outputs = self.model(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels\n",
        "                        )\n",
        "                        loss = outputs.loss\n",
        "\n",
        "                    self.scaler.scale(loss).backward()\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                else:\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels\n",
        "                    )\n",
        "                    loss = outputs.loss\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            print(f'\\nEpoch {epoch+1} - Average loss: {avg_loss:.4f}')\n",
        "\n",
        "            # Validation\n",
        "            val_loss = self.evaluate(val_loader)\n",
        "            print(f'Validation loss: {val_loss:.4f}')\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_loss += outputs.loss.item()\n",
        "\n",
        "        return total_loss / len(val_loader)\n",
        "\n",
        "    def correct_text(self, text):\n",
        "        self.model.eval()\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=128\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)"
      ],
      "metadata": {
        "id": "4N-Q4OXjGIsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CUSTOM DATASET CLASS\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class OCRCorrectionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        incorrect_text = row['incorrect_text']\n",
        "        correct_text = row['correct_text']\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            incorrect_text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                correct_text,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels['input_ids'].squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "2dS-lgg6GONT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HUGGINGFACE LOGIN -> IMPORTANT BEFORE TRAINING\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual Hugging Face API key\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "63tI3XAEjijA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGEXwd90EzUL"
      },
      "outputs": [],
      "source": [
        "#TRAINING\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive (if needed)\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Log into Hugging Face Hub\n",
        "    # notebook_login()\n",
        "\n",
        "    # Check GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "        print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "    else:\n",
        "        print('No GPU available, using CPU')\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare data\n",
        "    train_df, val_df = load_and_prepare_data()\n",
        "    print(f\"\\nTraining set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "    print(\"\\nColumn names:\", train_df.columns.tolist())\n",
        "\n",
        "    # Initialize trainer with mixed precision\n",
        "    trainer_obj = OCRTrainer(device=device, use_amp=True)\n",
        "\n",
        "    repo_name = \"movie-title-OCR-corrector-t5\"  # Replace with your desired repository name\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = OCRCorrectionDataset(train_df, trainer_obj.tokenizer)\n",
        "    val_dataset = OCRCorrectionDataset(val_df, trainer_obj.tokenizer)\n",
        "\n",
        "    # TrainingArguments for Hugging Face Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=repo_name,\n",
        "        per_device_train_batch_size=64,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=1,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,  # Log every 10 steps\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"steps\",  # Save checkpoints regularly\n",
        "        save_steps=500,  # Save a checkpoint every 500 steps\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=True,  # Push to Hugging Face Hub\n",
        "        resume_from_checkpoint=True,  # Allow resumption\n",
        "    )\n",
        "\n",
        "    # Hugging Face Trainer\n",
        "    trainer = Trainer(\n",
        "        model=trainer_obj.model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    train_results = trainer.train(resume_from_checkpoint=True)  # Resumes if checkpoint exists\n",
        "\n",
        "    # Save the model to Hugging Face Hub\n",
        "    trainer.save_model()\n",
        "    trainer.push_to_hub(commit_message=\"Final model after training\")\n",
        "\n",
        "    # Plot training loss if available\n",
        "    try:\n",
        "        plt.plot(train_results.history[\"loss\"], label=\"Training Loss\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Training Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    except AttributeError:\n",
        "        print(\"Training loss data not available for plotting.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "source": [
        "# Inference Test Cell\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_name = \"yashvoladoddi37/movie-title-OCR-corrector-t5\"  # Replace with your model name on Hugging Face\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ocr_correction_model'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Define device here\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)  # Assuming 'device' is defined\n",
        "\n",
        "# Test texts\n",
        "test_texts = [\n",
        "    \"The Godfthr\",\n",
        "    \"Pulp Ficton\",\n",
        "    \"Star Wars: The Las Jed\",\n",
        "    \"The Lord of the Rigns\",\n",
        "    \"Did you watch Avend3rs: Endgame or Star Warts? My favourites are The Dark Knigt, The Godfthr, and The Shawshank Redemtion. I also liked The Lord of the Rigns, Pulp Ficton, Inceptionn, and Interestellar. These are all classiccs.\",\n",
        "    \"Th3 Godfthr Star Warts The Dark Kn1gt The Shawshank Redemtion The Lord of the Rigns Pulp Ficton Av3nders: Endgame Inceptionn\"\n",
        "]\n",
        "\n",
        "# Perform inference\n",
        "for text in test_texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=256\n",
        "        )\n",
        "\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Original: {text}\")\n",
        "    print(f\"Corrected: {corrected_text}\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khFa-4rwHvTo",
        "outputId": "a1118056-4cd6-421d-ccfd-8146b101f493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Original: The Godfthr\n",
            "Corrected: The Godfather\n",
            "\n",
            "Original: Pulp Ficton\n",
            "Corrected: Pulp Fiction\n",
            "\n",
            "Original: Star Wars: The Las Jed\n",
            "Corrected: Star Wars: The Last Jedi\n",
            "\n",
            "Original: The Lord of the Rigns\n",
            "Corrected: The Lord of the Rings\n",
            "\n",
            "Original: Did you watch Avend3rs: Endgame or Star Warts? My favourites are The Dark Knigt, The Godfthr, and The Shawshank Redemtion. I also liked The Lord of the Rigns, Pulp Ficton, Inceptionn, and Interestellar. These are all classiccs.\n",
            "Corrected: Did you watch Avengers: Endgame or Star Wars? My favourites are The Dark Knigt, The Godfather, and The Shawshank Redemonstration\n",
            "\n",
            "Original: Th3 Godfthr Star Warts The Dark Kn1gt The Shawshank Redemtion The Lord of the Rigns Pulp Ficton Av3nders: Endgame Inceptionn\n",
            "Corrected: The Godfather Star Wars The Dark Knight The Shawshank Redemontion The Lord of the Rings Pulp Fiction Avengers: Endgame Inception\n",
            "\n"
          ]
        }
      ]
    }
  ]
}